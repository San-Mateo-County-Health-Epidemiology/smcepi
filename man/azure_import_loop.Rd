% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/azure-helpers.R
\name{azure_import_loop}
\alias{azure_import_loop}
\title{Write large datasets from R to Azure}
\usage{
azure_import_loop(azure_con, data, group_size, table_name, field_types = NULL)
}
\arguments{
\item{azure_con}{A DBI Connection object as returned by `dbConnect()`. See help text for `DBI::dbWriteTable` for more details}

\item{data}{A data.frame object you plan to import}

\item{group_size}{An integer that indicates the size of the import groups. Ex: a group_size of 50 would loop through the dataset and import 50 records at a time}

\item{table_name}{A string to specify the name of the table in Azure}

\item{field_types}{Optional parameter to pass a list of varchar(`n`) lengths for the Azure table. Can be created with the `smcepi::varchar_max()` function}
}
\value{
will import the data into azure and will return any errors from the `DBI::dbWriteTable` and the `DBI::dbAppendTable` functions
}
\description{
This is a wrapper function for the `DBI::dbWriteTable` and the `DBI::dbAppendTable` functions. This function breaks the dataset to be imported into smaller groups and then loops through the groups. The first group is used to create the table and subsequent groups are appended to the table.
}
\examples{
\dontrun{

azure_import_loop(azure_con = azure_con,
                  data = data,
                  group_size = 500,
                  table_name = "new_azure_table_name",
                  field_types = varchar_max)

}
}
